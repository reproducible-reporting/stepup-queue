{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to StepUp Queue","text":"<p>StepUp Queue is an experimental extension of StepUp Core to integrate queued jobs into a workflow. Currently, it only supports integration with SLURM, but it is designed to be extensible to other job schedulers.</p> <p>A simple example of a dataset created with StepUp Queue, is the Sodium Chloride Electrolyte Equilibrium Molecular Dynamics Simulations dataset on Zenodo.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes to StepUp Queue will be documented on this page.</p> <p>The format is based on Keep a Changelog, and this project adheres to Effort-based Versioning. (Changes to features documented as \u201cexperimental\u201d will not increment macro and meso version numbers.)</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":"<p>(no changes yet)</p>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>New <code>stepup removejobs</code> command to remove job directories,   by default only of failed jobs.   This command uses the same safeguards as <code>stepup clean</code>   in the upcoming StepUp Core 3.2 release, i.e.,   it only performs destructive actions when explicitly confirmed by the user   with the <code>--commit</code> flag.</li> <li>Detect unsupported scheduler directives in job scripts   (e.g., PBS, LSF, Cobalt) and raise an error.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Refactored <code>stepup canceljobs</code> to use the same safeguards as <code>stepup clean</code>   in the upcoming StepUp Core 3.2 release.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Corrected missing dependency and inconsistency with <code>.github/requirements-old.txt</code>.</li> <li>Filter jobs by status in <code>stepup canceljobs</code>,   so it only cancels jobs that are not done, unless the <code>--all</code> flag is used.</li> <li>Fixed mistake in regular expressions to detect forbidden <code>#SBATCH</code> options.</li> </ul>"},{"location":"changelog/#v1.0.7","title":"1.0.7 - 2025-12-07","text":"<p>Improved robustness for workflows with many concurrent jobs.</p>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Improved perpetual workflow example.</li> <li>Increased StepUp Core dependency to &gt;=3.1.4 because it fixes a bug that is likely to occur   in combination with StepUp Queue.</li> <li>Explicitly raise an error for array jobs, as these are not supported.</li> <li>More intuitive environment variables for polling.</li> <li>Retry <code>sbatch</code> on failure before giving up. (Default is 5 attempts with 1-2 minute delays.)</li> <li>Improved usage documentation and hints.</li> <li>Check that job scripts are executable and have a shebang line.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Improved robustness for workflow with many concurrent jobs, by using <code>sacct</code>   instead of <code>scontrol</code> to query job states.   This avoids the ambiguity that an unlisted job may either be pending or already finished long ago.   With <code>sact</code>, unlisted jobs are always (aboute to become) pending.</li> <li>Improved parsing of <code>#SBATCH</code> lines in job scripts.   To avoid confusion <code>#STBATCH -o/--output</code> and <code>#SBATCH -e/--error</code> will raise an error.   (StepUp Queue overrides these options internally to capture job output and error logs.)</li> <li>Fix parsing bug in <code>canceljobs</code> tool.</li> <li>Prevent infinite loop for jobs that are unlisted for too long.</li> <li>Make <code>stepup canceljobs</code> work correctly without arguments.</li> </ul>"},{"location":"changelog/#v1.0.6","title":"1.0.6 - 2025-11-30","text":"<p>Documentation updates and one bug fix.</p>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Document how to interrupt StepUp gracefully while jobs are running.</li> <li>Document convenient settings during workflow development or debugging.</li> <li>Increased the default value of <code>STEPUP_SBATCH_TIME_MARGIN</code> from 5 to 15 seconds.</li> <li>CI testing for Python 3.14 instead of 3.13.</li> <li>Smaller package size on PyPI.</li> <li>Increased StepUp Core dependency to &gt;=3.1.3 to ensure usage instructions work.</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Removed logging of potentially transient job states.</li> </ul>"},{"location":"changelog/#v1.0.5","title":"1.0.5 - 2025-05-23","text":""},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Replaced the old <code>STEPUP_QUEUE_RESUBMIT_CHANGED_INPUTS</code> environment variable   by the more powerful <code>STEPUP_QUEUE_ONCHANGE</code>.</li> </ul>"},{"location":"changelog/#v1.0.4","title":"1.0.4 - 2025-05-21","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Minor typo fix in slurm wrapper script.</li> <li>Improved example perpetual workflow job script.</li> </ul>"},{"location":"changelog/#v1.0.3","title":"1.0.3 - 2025-05-16","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed errors in the example job scripts.</li> <li>Improved handling of <code>scontrol</code> failures.</li> </ul>"},{"location":"changelog/#added_1","title":"Added","text":""},{"location":"changelog/#v1.0.2","title":"1.0.2 - 2025-05-14","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Option to specify the extension of the job script.</li> <li>Wrap all job scripts to record their return code.</li> <li>Detect when inputs of jobs have changed + optional resubmission.</li> <li>Option to load resource configurations before sbatch is called.</li> <li>More detailed examples, including a self-submitting workflow job.</li> </ul>"},{"location":"changelog/#v1.0.1","title":"1.0.1 - 2025-05-11","text":"<p>This is a minor cleanup release, mainly testing the release process.</p>"},{"location":"changelog/#v1.0.0","title":"1.0.0 - 2025-05-11","text":"<p>This is an initial and experimental release of StepUp Queue.</p>"},{"location":"changelog/#added_3","title":"Added","text":"<p>Initial release of StepUp Queue. The initial package is based on the <code>sbatch-wait</code> script from Parman. It was adapted to integrate well with StepUp Core 3. This release also features the <code>stepup canceljobs</code> tool, which was not present in Parman.</p>"},{"location":"development/","title":"Developer Notes","text":"<p>If you would like to contribute, please read CONTRIBUTING.md.</p>"},{"location":"development/#development-environment","title":"Development environment","text":"<p>If you break your development environment, you can discard it by running <code>git clean -dfX</code> and repeating the instructions below.</p> <p>A local installation for testing and development can be installed using the following commands:</p> <pre><code>git clone git@github.com:reproducible-reporting/stepup-queue.git\ncd stepup-queue\npre-commit install\npython -m venv venv\n</code></pre> <p>Put the following lines in <code>.envrc</code>:</p> <pre><code>source venv/bin/activate\nexport XDG_CACHE_HOME=\"${VIRTUAL_ENV}/cache\"\nexport STEPUP_DEBUG=\"1\"\nexport STEPUP_SYNC_RPC_TIMEOUT=\"30\"\n</code></pre> <p>Finally, run the following commands:</p> <pre><code>direnv allow\npip install -U pip\npip install -e .[dev]\npip install -e ../stepup-core --config-settings editable_mode=strict  # optional\n</code></pre>"},{"location":"development/#tests","title":"Tests","text":"<p>We use pytest, so you can run the tests as follows:</p> <pre><code>pytest -vv\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<p>The documentation is created using MkDocs. mike is used to manage documentation of different versions</p> <p>Edit the documentation Markdown files with a live preview by running:</p> <pre><code>mkdocs serve\n</code></pre> <p>(Keep this running.) Then open the live preview in your browser at http://127.0.0.1:8000/ and edit Markdown files in your IDE.</p> <p>Please, use Semantic Line Breaks because it facilitates reviewing documentation changes.</p>"},{"location":"development/#tutorial-example-outputs","title":"Tutorial Example Outputs","text":"<p>If you wish to regenerate the output of the examples, run <code>stepup</code> in the <code>docs</code> directory:</p> <pre><code>cd docs\nstepup\n</code></pre> <p>(Keep this running.) Then open the live preview in your browser: http://127.0.0.1:8000/ and edit Markdown files in your IDE.</p> <p>Please, use Semantic Line Breaks because it results in cleaner file diffs when editing documentation.</p>"},{"location":"development/#how-to-make-a-release","title":"How to Make a Release","text":"<ul> <li>Mark the release in <code>docs/changelog.md</code>.   Do not forget to extend the links at the bottom of the file.</li> <li>Make a new commit and tag it with <code>vX.Y.Z</code>.</li> <li>Trigger the PyPI GitHub Action: <code>git push origin main --tags</code>.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Requirements:</p> <ul> <li>POSIX operating system: Linux, macOS or WSL.   StepUp cannot run natively on Windows.</li> <li>Python \u2265 3.11</li> <li>Pip</li> <li>StepUp Core &gt;= 3.0.0</li> </ul> <p>It is assumed that you know how to use Pip. We recommend performing the installation in a Python virtual environment and activating such environments with direnv.</p> <p>StepUp Queue can be installed with:</p> <pre><code>pip install stepup-queue\n</code></pre>"},{"location":"license/","title":"License","text":""},{"location":"license/#source-code-license","title":"Source code license","text":"<p>StepUp Queue is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>StepUp Queue is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with this program. If not, see https://www.gnu.org/licenses/.</p>"},{"location":"license/#documentation-license","title":"Documentation license","text":"<p>StepUp Queue\u2019s documentation is distributed under the Creative Commons CC BY-SA 4.0 license.</p>"},{"location":"stepup.queue.api/","title":"stepup.queue.api","text":"<p>StepUp Queue API functions to build workflows.</p>"},{"location":"stepup.queue.api/#stepup.queue.api.sbatch","title":"<code>sbatch(workdir, *, ext='.sh', rc=None, inp=(), env=(), out=(), vol=(), onchange=None, optional=False, pool=None, block=False)</code>","text":"<p>Submit a SLURM job script.</p> <p>The following filename conventions are used in the given working directory:</p> <ul> <li><code>slurmjob{ext}</code> is the job script to be submitted.</li> <li><code>slurmjob.log</code> is StepUp Queue\u2019s log file keeping track of the job\u2019s status.</li> <li><code>slurmjob.out</code> is the job\u2019s output file (written by SLURM).</li> <li><code>slurmjob.err</code> is the job\u2019s error file (written by SLURM).</li> <li><code>slurmjob.ret</code> is the job\u2019s return code (written by a wrapper script).</li> </ul> <p>Hence, you can only have one job script per working directory, and it is strongly recommended to use meaningful directory names. Within the directory, try to use as much as possible exactly the same file names for all jobs.</p> <p>When the step is executed, it will submit the job or skip this if it was done before. If submitted, the step will wait until the job is finished. If already finished, the step will essentially be a no-op.</p> <p>See <code>step()</code> documentation in StepUp Core for all optional arguments. and the return value. Note that the <code>inp</code>, <code>out</code> and <code>vol</code> arguments are extended with the files mentioned above and that any additional files you specify are interpreted relative to the working directory.</p> <p>Parameters:</p> <ul> <li> <code>ext</code>               (<code>str</code>, default:                   <code>'.sh'</code> )           \u2013            <p>The filename extension of the jobscript. The full name is <code>f\"slurmjob{ext}\"</code>. Extensions <code>.log</code>, <code>.out</code>, <code>.err</code> and <code>.ret</code> are not allowed.</p> </li> <li> <code>rc</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A resource configuration to be executed before calling sbatch. This will be executed in the same shell, right before the sbatch command. For example, you can run <code>module swap cluster/something</code> or prepare other resources. If multiple instructions are needed, put them in a file, e.g. <code>rc.sh</code> and pass it here as <code>source rc.sh</code>. In this case, you usually also want to include <code>rc.sh</code> in the <code>inp</code> list.</p> </li> <li> <code>onchange</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Policy when a the inputs of a previously submitted job have changed. Must be one of <code>\"raise\"</code>, <code>\"resubmit\"</code> or <code>\"ignore\"</code>.</p> </li> </ul> Source code in <code>stepup/queue/api.py</code> <pre><code>def sbatch(\n    workdir: str,\n    *,\n    ext: str = \".sh\",\n    rc: str | None = None,\n    inp: Collection[str] | str = (),\n    env: Collection[str] | str = (),\n    out: Collection[str] | str = (),\n    vol: Collection[str] | str = (),\n    onchange: str | None = None,\n    optional: bool = False,\n    pool: str | None = None,\n    block: bool = False,\n):\n    \"\"\"Submit a SLURM job script.\n\n    The following filename conventions are used in the given working directory:\n\n    - `slurmjob{ext}` is the job script to be submitted.\n    - `slurmjob.log` is StepUp Queue's log file keeping track of the job's status.\n    - `slurmjob.out` is the job's output file (written by SLURM).\n    - `slurmjob.err` is the job's error file (written by SLURM).\n    - `slurmjob.ret` is the job's return code (written by a wrapper script).\n\n    Hence, you can only have one job script per working directory,\n    and it is strongly recommended to use meaningful directory names.\n    Within the directory, try to use as much as possible exactly the same file names for all jobs.\n\n    When the step is executed, it will submit the job or skip this if it was done before.\n    If submitted, the step will wait until the job is finished.\n    If already finished, the step will essentially be a no-op.\n\n    See `step()` documentation in StepUp Core for all optional arguments.\n    and the return value.\n    Note that the `inp`, `out` and `vol` arguments are extended\n    with the files mentioned above and that any additional files you specify\n    are interpreted relative to the working directory.\n\n    Parameters\n    ----------\n    ext\n        The filename extension of the jobscript.\n        The full name is `f\"slurmjob{ext}\"`.\n        Extensions `.log`, `.out`, `.err` and `.ret` are not allowed.\n    rc\n        A resource configuration to be executed before calling sbatch.\n        This will be executed in the same shell, right before the sbatch command.\n        For example, you can run `module swap cluster/something`\n        or prepare other resources.\n        If multiple instructions are needed, put them in a file, e.g. `rc.sh`\n        and pass it here as `source rc.sh`.\n        In this case, you usually also want to include `rc.sh` in the `inp` list.\n    onchange\n        Policy when a the inputs of a previously submitted job have changed.\n        Must be one of `\"raise\"`, `\"resubmit\"` or `\"ignore\"`.\n    \"\"\"\n    if ext == \"\":\n        ext = \".sh\"\n    elif ext[0] != \".\":\n        ext = f\".{ext}\"\n    if ext in [\".log\", \".out\", \".err\", \".ret\"]:\n        raise ValueError(f\"Invalid extension {ext}. The extension must not be .log, .out or .err.\")\n    action = \"sbatch\"\n    if ext != \".sh\":\n        action += f\" {ext}\"\n    if rc is not None:\n        action += f\" --rc={shlex.quote(rc)}\"\n    if onchange is not None:\n        if onchange not in [\"raise\", \"resubmit\", \"ignore\"]:\n            raise ValueError(f\"Invalid onchange policy {onchange}.\")\n        action += f\" --onchange={onchange}\"\n    return step(\n        action,\n        inp=[f\"slurmjob{ext}\", *string_to_list(inp)],\n        env=env,\n        out=[\"slurmjob.out\", \"slurmjob.err\", \"slurmjob.ret\", *string_to_list(out)],\n        vol=[\"slurmjob.log\", *string_to_list(vol)],\n        workdir=workdir,\n        optional=optional,\n        pool=pool,\n        block=block,\n    )\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#the-sbatch-function","title":"The <code>sbatch()</code> Function","text":"<p>If you want to submit a job to the queue as part of a StepUp workflow, you must first prepare a directory with a job script called <code>slurmjob.sh</code>. This can be either a static file or the output of a previous step in the workflow. The function <code>sbatch()</code> will then submit the job to the queue. For simplicity, the following example assumes that the job script is static:</p> <pre><code>from stepup.core.api import static\nfrom stepup.queue.api import sbatch\n\nstatic(\"compute/\", \"compute/slurmjob.sh\")\nsbatch(\"compute/\")\n</code></pre> <p>All arguments to the <code>sbatch</code> command of SLURM must be included in the <code>slurmjob.sh</code> script with <code>#SBATCH</code> directives. There are some constraints to keep in mind:</p> <ul> <li>You can only submit one job from a given directory.</li> <li>The job script must be called <code>slurmjob.sh</code>,   or may have a different extension if it is not a shell script.</li> <li>The job script must be executable and have a shebang line (e.g. <code>#!/usr/bin/env bash</code>).</li> <li>The job script cannot specify an output or error file.   These will be specified by the <code>sbatch()</code> function   as <code>slurmjob.out</code> and <code>slurmjob.err</code>, respectively.</li> <li>Array jobs are not supported.   Presence of the <code>--array</code> option in the <code>sbatch</code> command will raise an exception.</li> </ul> <p>When the workflow is executed, the <code>sbatch</code> step will submit the job to the queue. It will then wait for the job to complete, just like <code>sbatch --wait</code>. Unlike <code>sbatch --wait</code>, it can also wait for a previously submitted job to complete. This is useful when the workflow gets killed for some reason, e.g. due to a wall time limit.</p> <p>The current status of the job is stored in the <code>slurmjob.log</code> file, which StepUp Queue both reads and writes. When you restart StepUp and <code>slurmjob.log</code> exists for a given <code>sbatch()</code> step, the job is not resubmitted; instead, StepUp waits for the existing job to finish. To force a job to be resubmitted, you must delete <code>slurmjob.log</code> and manually cancel the corresponding running job, before restarting StepUp. Deleting <code>slurmjob.log</code> without cancelling the job will cause inconsistencies that StepUp cannot detect.</p> <p>If the job\u2019s inputs change and StepUp is restarted, you can control how this situation is handled using the <code>STEPUP_QUEUE_ONCHANGE</code> environment variable or the <code>onchange</code> argument of <code>sbatch()</code>:</p> <ol> <li><code>onchange=\"raise\"</code> (default):     Raises an exception and aborts the workflow.     This is the safest option, ensuring the workflow does not continue with inconsistent data.</li> <li><code>onchange=\"resubmit\"</code>:     Cancels any running job and removes it from the queue,     then resubmits the job with the new inputs.     Old outputs are not deleted before resubmission;     it is assumed that your job script will handle any necessary cleanup.</li> <li><code>onchange=\"ignore\"</code>:     Does not resubmit the job; the workflow continues using any existing outputs.     This is useful if input changes do not affect outputs,     e.g., updating the job script to request more resources.     If outputs are missing but <code>slurmjob.log</code> exists, the step will fail.     If you manually remove <code>slurmjob.log</code> and cancel the running job,     the job will be resubmitted with the new inputs.     Use this option with caution, as it can lead to inconsistent workflow data.</li> </ol>"},{"location":"usage/#examples","title":"Examples","text":"<ul> <li> <p>A simple example with static and dynamically generated job scripts   can be found in the <code>examples/slurm-basic/</code>.</p> </li> <li> <p>The example <code>examples/slurm-perpetual/</code>   shows how to run StepUp itself as a job in the queue,   which cancels and submits itself again when nearing the wall time limit,   if the workflow has not yet completed.</p> </li> </ul>"},{"location":"usage/#stopping-the-workflow-gracefully","title":"Stopping the Workflow Gracefully","text":"<p>When running StepUp interactively, and the workflow is just waiting for jobs to complete, you can stop it gracefully by pressing <code>q</code> twice. This will interrupt the worker processes that are waiting for SLURM jobs to be completed. The actual SLURM jobs will continue to run. You can later restart StepUp to continue waiting for the jobs to complete.</p> <p>If you are running StepUp itself also as a SLURM job, you have no keyboard interaction to stop it. In this case, you can achieve the same effect by logging into the node where StepUp is running, and running the <code>stepup shutdown</code> command twice in the directory where StepUp is running. If this is needed often, e.g. when debugging, you can automate this with a script. The following is a simple example, which you may need to adapt for your setup:</p> <pre><code>#!/usr/bin/bash\nif [ -z \"$1\" ]; then echo \"Usage: $0 &lt;jobid&gt;\"; exit 1; fi\nJOBID=$(echo \"${1}\" | tr -dc '0-9')\nNODE=$(squeue -j ${JOBID} -h -o %N)\nWORKDIR=$(squeue -j ${JOBID} -h -o %Z)\nSCRIPT=\"cd ${WORKDIR}/.. &amp;&amp; source activate &amp;&amp; cd ${WORKDIR} &amp;&amp; stepup shutdown &amp;&amp; stepup shutdown\"\nCOMMAND=\"bash -l -c '${SCRIPT}'\"\necho \"Executing on node ${NODE}: ${COMMAND}\"\nssh ${NODE} \"${COMMAND}\"\n</code></pre> <p>The script takes a SLURM job ID (or the out file, like <code>slurm-&lt;jobid&gt;.out</code>) as an argument, determines the node and working directory of the job, and runs <code>stepup shutdown</code> twice in that directory via <code>ssh</code>. The way the software environment is activated may differ from your setup.</p> <p>A less sophisticated approach is to simply cancel the StepUp job via <code>scancel &lt;jobid&gt;</code>. This will stop StepUp immediately, but it may not have the chance to write its state to disk. Normally, this should be fine, as it uses SQLite, which is robust against crashes.</p>"},{"location":"usage/#killing-running-jobs","title":"Killing Running Jobs","text":"<p>StepUp Queue will not cancel any jobs when the workflow is interrupted. It is quite common for a workflow to be interrupted by accident or for technical reasons. In this case, it would be inefficient to also cancel running jobs, which may still be doing useful work. Instead, jobs continue to run, and you can restart the StepUp workflow to pick up where it left off.</p> <p>If you want to cancel running SLURM jobs, typically after interrupting StepUp, you can run the following command:</p> <pre><code>stepup canceljobs dir/to/running/jobs\n</code></pre> <p>This command will recursively look for all <code>slurmjob.log</code> files in the given paths (or the current directory if no paths are given), extract the corresponding job IDs of running jobs, and generate <code>scancel</code> commands to cancel them. After each <code>scancel</code> command, the path of the <code>slurmjob.log</code> file and the job status are added as a comment. For example, the output may look like this:</p> <pre><code>scancel 123456  # path/to/job1/slurmjob.log RUNNING\nscancel -M somecluster 123457  # path/to/job2/slurmjob.log PENDING\n</code></pre> <p>By default, this command will not perform any destructive actions and will only print the <code>scancel</code> commands that would be executed. You can pass the <code>--commit</code> option to actually execute the <code>scancel</code> commands. Alternatively, you can select a subset of jobs to cancel with <code>grep</code>, for example:</p> <pre><code>stepup canceljobs dir/to/running/jobs | grep \"filename_pattern\"\n</code></pre> <p>Make sure you always check the generated <code>scancel</code> commands before executing them.</p>"},{"location":"usage/#removing-directories-of-cancelled-or-failed-jobs","title":"Removing Directories of Cancelled or Failed Jobs","text":"<p>After a job was cancelled or has failed, the corresponding files are not removed automatically. This is to allow for inspection of the job\u2019s output and error files for debugging purposes. You can remove the directories of cancelled or failed jobs by running the following command:</p> <pre><code>stepup removejobs dir/to/jobs\n</code></pre> <p>This command will recursively look for all <code>slurmjob.log</code> files in the given paths (or the current directory if no paths are given), check the status of the corresponding jobs, and remove the directories of jobs that have ended but were not successful.</p> <p>By default, this command will not perform any destructive actions and will only print the remove commands that would be executed. You can pass the <code>--commit</code> option to actually remove the directories.</p>"},{"location":"usage/#useful-settings-when-developing-workflows","title":"Useful Settings when Developing Workflows","text":"<p>When developing and testing workflows that use <code>sbatch()</code>, it can be useful to configure StepUp to be less rigorous about reproducibility and file cleaning. The following environment variables can help with this:</p> <pre><code># Do not remove outdated output files after a successful completion of the workflow.\nexport STEPUP_CLEAN=0\n# Ignore changes in inputs to sbatch() steps and do not resubmit jobs.\n# Just assume that existing outputs are still valid, despite changed inputs.\nexport STEPUP_QUEUE_ONCHANGE=\"ignore\"\n</code></pre> <p>These settings are not recommended for production workflows.</p>"},{"location":"usage/#hints-for-writing-job-scripts","title":"Hints for writing Job Scripts","text":"<p>When writing job scripts for any type of workflow (not only StepUp), and the cluster is using a distributed file system (like IBM Spectrum Scale or Lustre), you should not assume that files created by one job are immediately visible to subsequent jobs. When the cluster load is high, there can be significant delays in the file metadata synchronization, sometimes several minutes. To mitigate this issue, you can add a waiting function to your job scripts, like this:</p> <pre><code>wait_for_file() {\n  # Waiting function to deal with file synchronization issues.\n  local file=\"$1\"\n  local timeout=\"${2:-600}\"   # timeout in seconds (default 10 min)\n  local interval=\"${3:-5}\"    # poll interval in seconds (default 5 sec)\n\n  local elapsed=0\n\n  while [[ ! -e \"$file\" ]]; do\n    if (( elapsed &gt;= timeout )); then\n      echo \"ERROR: Timeout waiting for file: $file\" &gt;&amp;2\n      return 1\n    fi\n    sleep \"$interval\"\n    (( elapsed += interval ))\n  done\n\n  return 0\n}\n</code></pre> <p>Further down in the job script, this function can be used as follows:</p> <pre><code># Wait for input.dat to appear or exit with error after timeout.\nwait_for_file \"input.dat\" || exit 1\n</code></pre>"},{"location":"usage/#technical-details-of-stepup-queues-interaction-with-slurm","title":"Technical Details of StepUp Queue\u2019s interaction with SLURM","text":"<p>The timestamps in the <code>slurmjob.log</code> are inherently inaccurate (order one minute) due to the caching mechanism of SLURM. Checking more job states frequently with <code>sacct</code> (SLURM accounting tool) would also put too much load on SLURM, which is not desirable. StepUp Queue will therefore run <code>sacct</code> only occasionally and cache its output on disk. Furthermore, a <code>slurmjob.log</code> file is written for each job to keep track of its submission time, job ID, and previous states.</p> <p>The status of the jobs is inferred from <code>sacct -o 'jobidraw,state' -PXn</code>, if relevant with a <code>--cluster</code> argument. In addition, a configurable <code>-S</code> argument is passed to <code>sacct</code>. Its output is cached in a subdirectory <code>.stepup/queue</code> of the workflow root. The cached result is reused by all <code>sbatch</code> actions, so the number of <code>sacct</code> calls is independent of the number of jobs running in parallel.</p> <p>The time between two <code>sacct</code> calls (per cluster) can be controlled with the <code>STEPUP_SBATCH_CACHE_TIMEOUT</code> environment variable, which is <code>\"30\"</code> (seconds) by default. Increase this value if you want to reduce the burden on SLURM.</p> <p>The cached output of <code>sacct</code> is checked by the <code>sbatch</code> actions with a randomized polling interval. If any of these actions notices that the cached file is too old, it will acquire a lock on the cache file and update it by calling <code>sacct</code>. The randomization guarantees that concurrent calls to <code>sacct</code> (for multiple clusters) will not all coincide. The polling time can be controlled with two additional environment variables:</p> <ul> <li><code>STEPUP_SBATCH_POLLING_MIN</code> = the minimal polling interval in seconds, default is <code>10</code>.</li> <li><code>STEPUP_SBATCH_POLLING_MAX</code> = the maximal polling interval in seconds, default is <code>20</code>.</li> </ul> <p>By default, <code>sacct</code> queries job information of the last 7 days. You can change this by setting the <code>STEPUP_SACCT_START_TIME</code> environment variable to a different value understood by <code>sacct -S</code>, e.g., <code>now-4days</code> or <code>2025-01-01T00:00:00</code>.</p> <p>To avoid an infinite loop for jobs that are unlisted for too long, a job is considered to be failed if it is not listed for more than <code>STEPUP_SBATCH_UNLISTED_TIMEOUT</code> seconds, which is <code>600</code> (10 minutes) by default.</p> <p>Sometimes, job submission with <code>sbatch</code> can fail due to transient issues, such as temporary communication problems with the SLURM controller. To improve robustness, StepUp Queue will retry the <code>sbatch</code> command a number of times before giving up. You can control the retry behavior with the following environment variables:</p> <ul> <li><code>STEPUP_SBATCH_RETRY_NUM</code>: Number of retry attempts (default is <code>5</code>).</li> <li><code>STEPUP_SBATCH_RETRY_DELAY_MIN</code>: Minimum delay between retries in seconds (default is <code>60</code>).</li> <li><code>STEPUP_SBATCH_RETRY_DELAY_MAX</code>: Maximum delay between retries in seconds (default is <code>120</code>).</li> </ul>"},{"location":"examples/slurm-basic/","title":"Basic SLURM example","text":"<p>The latest version of this example can be found at: https://github.com/reproducible-reporting/stepup-queue/tree/main/docs/examples/slurm-basic/</p> <p>This example shows how to use StepUp to run job scripts, which can be either manually written (static) or generated from a template (dynamic). Since these jobs only take a few seconds and don\u2019t perform any computations, they allow for a quick demonstration of StepUp Queue\u2019s features.</p>"},{"location":"examples/slurm-basic/#files","title":"Files","text":"<pre><code>.\n\u251c\u2500\u2500 dynamic-template.sh\n\u251c\u2500\u2500 fail\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurmjob.sh\n\u251c\u2500\u2500 pass\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurmjob.py\n\u251c\u2500\u2500 plan.py\n\u2514\u2500\u2500 README.md\n</code></pre> <p><code>plan.py</code> is a Python script that defines the workflow:</p> <pre><code>#!/usr/bin/env python3\n\nfrom stepup.core.api import mkdir, render_jinja, static\nfrom stepup.queue.api import sbatch\n\n# Two examples of a static job script, i.e. already present on disk.\nstatic(\"pass/\", \"pass/slurmjob.py\")\nsbatch(\"pass\", ext=\".py\")\nstatic(\"fail/\", \"fail/slurmjob.sh\")\nsbatch(\"fail\")\n\n# Example of job scripts generated from a template.\nstatic(\"dynamic-template.sh\")\nfor i in range(1, 4):\n    mkdir(f\"dynamic{i}/\")\n    render_jinja(\"dynamic-template.sh\", {\"field\": i}, f\"dynamic{i}/slurmjob.sh\")\n    # You can use the rc option to load an environment before calling sbatch.\n    # Use this only if it cannot be done in the job script itself.\n    sbatch(f\"dynamic{i}/\", rc=\"module swap cluster/doduo\")\n</code></pre> <p>The job <code>fail/slurmjob.sh</code> is a static job script that fails with a non-zero exit code, which is correctly handled by StepUp Queue:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name fail\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n\necho \"This job will fail\"\nexit 1\n</code></pre> <p>The job <code>pass/slurmjob.py</code> shows how to write a Job script in Python:</p> <pre><code>#!/usr/bin/env python3\n#SBATCH --job-name pass\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n\nfrom time import sleep\n\nprint(\"Hello from static job\")\nsleep(5)\nprint(\"Goodbye from static job\")\n</code></pre> <p>The file <code>dynamic-template.sh</code> is a template from which actual job scripts are generated:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name 'dyn{{ field }}'\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n\necho \"Hello from dynamic job {{ field }}\"\nsleep 5\necho \"Goodbye from dynamic job {{ field }}\"\n</code></pre> <p>Note that <code>render_jinja</code> can be used to render any kind of text-based file from a template, such as inputs to computational tools, configuration files, etc.</p>"},{"location":"examples/slurm-perpetual/","title":"Perpetual SLURM Workflow Job","text":"<p>The latest version of this example can be found at: https://github.com/reproducible-reporting/stepup-queue/tree/main/docs/examples/slurm-perpetual/</p> <p>For extensive workflows, it is often useful to submit the workflow itself to the queue as a job. It is generally preferred to run the workflow on a compute node of the cluster, as this allows for better resource management and prevents overloading the login node. However, most clusters impose a limit on the maximum wall time of a job, which can result in the workflow job being interrupted. This example shows how to work around this limitation by using a perpetual self-submitting job.</p> <p>At the start of the job, a background process is launched that will end StepUp before the wall time limit is reached if StepUp has not ended on its own. When StepUp is interrupted, a temporary file is created. This file is later used as a signal that the workflow job needs to be resubmitted. This technique can be used with any type of job and is not specific to StepUp.</p> <p>Here, we use a very short runtime to quickly demonstrate StepUp Queue\u2019s features. In practice, you can let the StepUp job run for several hours or even days at a time, and stop it about 30 minutes before the wall time limit is reached.</p>"},{"location":"examples/slurm-perpetual/#files","title":"Files","text":"<pre><code>.\n\u251c\u2500\u2500 plan.py\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 step1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurmjob.sh\n\u251c\u2500\u2500 step2\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurmjob.sh\n\u2514\u2500\u2500 workflow.sh\n</code></pre> <p><code>plan.py</code> is a Python script that defines the workflow:</p> <pre><code>#!/usr/bin/env python3\n\nfrom stepup.core.api import static\nfrom stepup.queue.api import sbatch\n\nstatic(\"step1/\", \"step1/slurmjob.sh\", \"step2/\", \"step2/slurmjob.sh\")\nsbatch(\"step1/\", out=\"../intermediate.txt\")\nsbatch(\"step2/\", inp=\"../intermediate.txt\")\n</code></pre> <p><code>step1/slurmjob.sh</code> is the first SLURM job:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name step1\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=00:02:00\n\n# Give the CPU a break...\nsleep 30\necho Done &gt; ../intermediate.txt\n</code></pre> <p><code>step2/slurmjob.sh</code> is the second SLURM job:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name step2\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n\n#SBATCH --time=00:02:00\n\n# Give the CPU a break...\nsleep 30\ncat ../intermediate.txt\n</code></pre> <p><code>workflow.sh</code> is the SLURM job script that runs the workflow:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name stepup\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --output=stepup-%j.out\n\n# The SBATCH parameters in this example are kept minimal for demonstration purposes.\n# In production, they need to be scaled up appropriately.\n# For example, for NWORKER=100, reasonable settings would be:\n# --cpus-per-task=8 --time=12:00:00 --mem=16G\n\n#SBATCH --cpus-per-task=1\n#SBATCH --time=00:01:00\n#SBATCH --mem=4G\n\n# Number of concurrent StepUp workers, which corresponds to the number of\n# concurrently submitted jobs in the Slurm queue:\nNWORKER=5\n\n# Time-out settings\n# SOFT: In production, 1800 seconds (before the wall limit) is reasonable.\nexport STEPUP_SHUTDOWN_TIMEOUT_SOFT=30\n# HARD: In production, 600 seconds (before the wall limit) is reasonable.\nexport STEPUP_SHUTDOWN_TIMEOUT_HARD=10\n\necho \"StepUp workflow job starts:\" $(date)\n\n# If needed, load required modules and activate a relevant virtual environment.\n# For example:\n# module load Python/3.12.3\n# activate venv/bin/activate\n\n# Create a temporary directory to store a file that will be used as a flag\n# to indicate that resubmission is needed.\nSTEPUP_QUEUE_FLAG_DIR=$(mktemp -d)\necho \"Created temporary directory: $STEPUP_QUEUE_FLAG_DIR\"\ntrap 'rm -rv \"$STEPUP_QUEUE_FLAG_DIR\"' EXIT\n\n# Start a background process that will end stepup near the wall time limit.\n# The first shutdown will wait for running steps to completed.\n# The second will forcefully terminate remaining running steps.\necho \"Starting background process to monitor wall time.\"\n(\n    sleep $((${SLURM_JOB_END_TIME} - ${SLURM_JOB_START_TIME} - ${STEPUP_SHUTDOWN_TIMEOUT_SOFT}))\n    touch ${STEPUP_QUEUE_FLAG_DIR}/resubmit\n    stepup shutdown\n    sleep ${STEPUP_SHUTDOWN_TIMEOUT_HARD}\n    stepup shutdown\n) &amp;\nBGPID=$!\ntrap \"kill $BGPID\" EXIT\n\necho \"Starting stepup with a maximum of ${NWORKER} concurrent jobs.\"\nstepup boot -n ${NWORKER}\n# This means that at most ${NWORKER} jobs will be submitted concurrently.\n# You can adjust the number of workers based on your needs.\n# In fact, because this example is simple, a single worker would be sufficient.\n# Note that the number of workers is unrelated to the single core used by this workflow script.\n\n# Use the temporary file to determine if the workflow script must be resubmitted.\necho \"Checking if stepup was forcibly stopped.\"\nif [ -f ${STEPUP_QUEUE_FLAG_DIR}/resubmit ]; then\n    echo \"Resubmitting job script to let StepUp finalize the workflow.\"\n    sbatch workflow.sh\nelse\n    echo \"Stepup stopped by itself.\"\nfi\n\necho \"StepUp workflow job ends:\" $(date)\n</code></pre>"}]}